# HDFS——核心参数
## NameNode内存配置
### NameNode内存计算
NameNode中存储`每个文件块的元数据`大概占用`150byte`，计算NameNode能存储的`文件块数量 = NameNode内存总量 / 150byte`
```
    案例：NameNode节点的RAM = 128GB
        
        128 * 1024 * 1024 * 1024 / 150 ≈ 9.1亿
         GB    MB     KB    Byte
```
### NameNode内存配置
#### Hadoop2.x系列
NameNode内存默认2000m，如果服务器内存4G，NameNode内存可以配置3g（NameNode内存 = 机器内存 * 75%）。在hadoop-env.sh文件中配置如下。
```shell script
# 初始化NameNode的内存容量
HADOOP_NAMENODE_OPTS=-Xmx3072m
```
#### Hadoop3.x系列
- hadoop-env.sh中描述Hadoop的内存是动态分配的
```shell script
# The maximum amount of heap to use (Java -Xmx).  If no unit
# is provided, it will be converted to MB.  Daemons will
# prefer any Xmx setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MAX=

# The minimum amount of heap to use (Java -Xms).  If no unit
# is provided, it will be converted to MB.  Daemons will
# prefer any Xms setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MIN=
HADOOP_NAMENODE_OPTS=-Xmx102400m
```
- 查看NameNode占用内存
```shell script
# 查看java进程列表
jps
# 根据NameNode进程号查看进程信息
jmap -heap <NameNodeProjectId>
```
- 查看DataNode占用内存
```shell script
# 根据DataNode进程号查看进程信息
jmap -heap <DataNodeProjectId>
```
##### 动态设置内存
经验参考：https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb
- NameNode最小值1G，每增加100 * 0000个block，增加1G内存
- DataNode最小值4G，block数或者副本数升高，应该相应调大DataNode的容量。一个DataNode上的副本总数低于400 * 10000，调为4G，超过400 * 10000，每增加100 * 10000，就增加1G内存。
- 具体修改：hadoop-env.sh
```shell script
# NameNode动态配置
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
# DataNode动态配置
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```
## NameNode心跳并发配置
```
             ---------         ---------         ---------         
            | client1 |       | client2 |       | client3 | 
             ---------         ---------         ---------
                |________________|___________________|
                                 |
                                 |
                                 |
                         --------|-------    
                        |   -----V----   |    NameNode准备多线程合适
                    ----|->| NameNode |<-|--------------------------------------------
                    |   |   ----------   |        |         ----------------         |         ----------------
               心跳 |   |    hadoop1     |        |        |    hadoop2     |        |        |    hadoop3     |
                    |   |   ----------   |        |        |   ----------   |        |        |   ----------   |        
                    ----|--| DataNode |  |        ---------|--| DataNode |  |        ---------|--| DataNode |  |
                        |   ----------   |       心跳      |   ----------   |       心跳      |   ----------   |
                         ----------------                   ----------------                   ----------------


```
- 设置hdfs-site.xml配置文件
```xml
<!--
    NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。
    对于大集群或者有大量客户端的集群来说，通常需要增大该参数。默认值是10。
-->
<property>
    <name>dfs.namenode.handler.count</name>
    <value>21</value>
</property>
```
- 企业经验：dfs.namenode.handler.count=20 * log(clustersize)，比如集群规模（DataNode台数）为3台时，此参数设置为21。
## 开启回收站配置
开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。
### 回收站工作机制
```
                                    -----------------------------------
                                   | 检查回收站的间隔时间              |
                                   | fs.trash.checkpoint.interval = 10 |
                                    -----------------------------------
                                                    |
                                                    |
                                                    |
                                                    V
                                         -------------------------
                                        |         回收站          |
      ---------------                   |     ---------------     |
     |  被删除的文件 | -----------------|--->|  已删除的文件 |    | 
      ---------------                   |     ---------------     |
                                        |     设置文件存活时间    |
                                        | fs.trash.interval = 60  |
                                         -------------------------
```
### 开启回收站功能参数说明
- 默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间。
- 默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。
- 要求fs.trash.checkpoint.interval <= fs.trash.interval。
### 启用回收站
修改core-site.xml，配置垃圾回收时间为1分钟。
```xml
<property>
    <name>fs.trash.interval</name>
    <value>1</value>
</property>
```
### 查看回收站
回收站目录在HDFS集群中的路径：/user/atguigu/.Trash/….
### 命令删除文件
```shell script
# 执行删除命令
hadoop fs -rm -r /user/atguigu/input
```
### 恢复回收站数据
```shell script
# 只能从回收站目录把已删除的文件移动出来
hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input
```
### 注意点
- 通过网页上直接删除的文件也不会走回收站。
- 通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站
```java
Trash trash = New Trash(conf);
trash.moveToTrash(path);
``` 
# HDFS——多目录
## NameNode
- NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。
- 配置方式：在hdfs-site.xml文件中添加如下内容
```xml
<property>
     <name>dfs.namenode.name.dir</name>
     <value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>
```
- 注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发
## DataNode
- DataNode可以配置成多个目录（数据盘扩容），每个目录存储的数据不一样（数据不是副本）
- 配置方式：在hdfs-site.xml文件中添加如下内容
```xml
<property>
     <name>dfs.datanode.data.dir</name>
     <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>
```
- 注意：hdfs没有对DataNode进行自动负载均衡策略，所以新加入的DataNode需要手动执行集群的数据均衡操作
## 集群数据均衡之磁盘间数据均衡
生产环境，由于硬盘空间不足，往往需要后面陆续增加硬盘块（DataNode目录）。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x新特性）
```
               ---------------- 
              |    hadoop1     |                    
              |   ----------   |
              |  | DataNode1 | |               ----------------                ----------------
              |   ----------   |              |    hadoop2     |              |    hadoop3     |
              |   ----------   |              |   -----------  |              |   -----------  |
              |  | DataNode2 | |              |  | DataNode |  |              |  | DataNode |  |
              |   -----------  |              |   -----------  |              |   -----------  |
               ----------------                ----------------                ----------------
```
- 生成均衡计划（hadoop1和hadoop2只有一块磁盘，不会生成计划）
```shell script
# HadoopNode为节点名称：hadoop1、hadoop2、hadoop3
hdfs diskbalancer -plan <HadoopNone>
```
- 执行均衡计划
```shell script
hdfs diskbalancer -execute <HadoopNode>.plan.json
```
- 查看当前均衡任务的执行情况
```shell script
hdfs diskbalancer -query <HadoopNode>
```
- 取消均衡任务
```shell script
hdfs diskbalancer -cancel <HadoopNode>.plan.json
```
# HDFS——集群扩容及缩容
## 白名单
- 白名单：表示在白名单的主机IP地址可以，用来存储数据。
- 企业中：配置白名单，可以尽量防止黑客恶意访问攻击。
```
         -----------------------------------------------------------
        |                         白名单                            |
        |      ----------------                                     |
        |     |    hadoop1     |                                    |
        |     |   ----------   |                                    |
        |     |  | DataNode1 | |               ----------------     |           ----------------
        |     |   ----------   |              |    hadoop2     |    |          |    hadoop3     |
        |     |   ----------   |              |   -----------  |    |          |   -----------  |
        |     |  | DataNode2 | |              |  | DataNode |  |    |          |  | DataNode |  |
        |     |   -----------  |              |   -----------  |    |          |   -----------  |
        |      ----------------                ----------------     |           ----------------
         -----------------------------------------------------------

注意：hadoop1和hadoop2可以用来存储数据，hadoop3不能用来存储数据
```
### 白名单配置流程
- 在NameNode节点的.../hadoop-3.1.3/etc/hadoop目录下创建whitelist文件
```shell script
# 创建白名单
vim whitelist
# 把集群中的hadoop1和hadoop2节点添加到白名单中
hadoop1
hadoop2
```
- 在hdfs-site.xml配置文件中增加dfs.hosts配置参数
```xml
<!-- 白名单 -->
<property>
     <name>dfs.hosts</name>
     <value>.../hadoop-3.1.3/etc/hadoop/whitelist</value>
</property>
```
- 分发配置文件whitelist，hdfs-site.xml
```shell script
xsync hdfs-site.xml whitelist
```
- `第一次`添加白名单必须`重启集群`，否则只需要刷新NameNode节点即可
```shell script
# 刷新NameNode
hdfs dfsadmin -refreshNodes
```
- 在web浏览器上查看DataNode情况，http://<HadoopNode>:9870/dfshealth.html#tab-datanode
- 注意：在`白名单之外`的机器上执行`上传数据会失败`
## 服务器扩容
### 需求
随着公司`业务增长`，数据量越来越大，原有的数据节点的`容量`已经`不能满足`存储数据的`需求`，需要在原有集群基础上`动态添加新的数据节点`。
### 环境准备
- 安装jdk
- 安装hadoop
- 初始化环境变量
- 分配hadoop配置文件
- 配置新机器和集群其他机器的ssh无秘登录
### 新机器加入集群
直接启动DataNode，即可关联到集群
```shell script
# 直接在新机器上启动DataNode
hdfs --daemon start datanode
# 直接在新机器上启动nodemanager
yarn --daemon start nodemanager
```
### 新机器加入白名单
- 在白名单whitelist中增加新机器，并重启集群
- 分发白名单
- 刷新NameNode
### 测试新节点
在新节点上向hdfs上传文件
```shell script
# 把test.txt文件上传到hdfs的根目录
hadoop fs -put .../test.txt /
```
## 服务器间数据均衡
思考：如果集群节点间的数据分布不均衡，有的节点爆满，有的节点空置，这种情况怎么处理呢？
### 企业经验
- 在企业开发中，由于`数据本地性`原则，经常提交任务的节点，它的数据量比较大，这样就会节点间数据分布不均衡。
- 另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。
```
               ----------------                ----------------                ----------------
              |     hadoop1    |              |    hadoop2     |              |    hadoop3     |
              |  ------------  |              |  -----------   |              |   -----------  |
              | |DataNode 10G| |              | |DataNode 5G|  |              |  |DataNode 1G| |
              |  ------------  |              |  -----------   |              |   -----------  |
               ----------------                ----------------                ----------------

  注意：每个机器上的数据量分布极度不均匀，所以需要执行集群均衡策略
```
### 开启数据均衡命令
```shell script
.../sbin/start-balancer.sh -threshold 10
```
对于参数10，代表的是集群中各个节点的`磁盘空间利用率`相差不超过10%，可根据实际情况进行调整。
### 关闭数据均衡命令
```shell script
.../sbin/stop-balancer.sh
```
注意：由于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，所以尽量不要在NameNode上执行start-balancer.sh，而是找一台比较空闲的机器。
## 黑名单退役服务器
- 黑名单：表示在黑名单的机器不可以用来存储数据。
- 企业中：配置黑名单，用来退役服务器。
```
               ---------------- 
              |    hadoop1     |                                         ----------------------------
              |   ----------   |                                        |           黑名单           |
              |  | DataNode1 | |               ----------------         |       ----------------     |
              |   ----------   |              |    hadoop2     |        |      |    hadoop3     |    |
              |   ----------   |              |   -----------  |        |      |   -----------  |    |
              |  | DataNode2 | |              |  | DataNode |  |        |      |  | DataNode |  |    |
              |   -----------  |              |   -----------  |        |      |   -----------  |    |
               ----------------                ----------------         |       ----------------     |
                                                                         ----------------------------
```
### 退役服务器步骤
- 编辑.../hadoop-3.1.3/etc/hadoop目录下的blacklist文件
```shell script
# 创建黑名单文件blacklist
vim blacklist
# 把退役服务器加入黑名单blacklist
hadoop3
```
- 注意：如果白名单中没有配置，需要在hdfs-site.xml配置文件中增加dfs.hosts配置参数
```xml
<!-- 黑名单 -->
<property>
     <name>dfs.hosts.exclude</name>
     <value>.../hadoop-3.1.3/etc/hadoop/blacklist</value>
</property>
```
- 分发配置文件blacklist，hdfs-site.xml
```shell script
xsync hdfs-site.xml blacklist
```
- 第一次添加黑名单必须重启集群，不是第一次，只需要刷新NameNode节点即可
```shell script
hdfs dfsadmin -refreshNodes
```
- 检查Web浏览器，退役节点的状态为`decommission in progress`（退役中），说明数据节点正在复制块到其他节点
```
    节点状态：
        in service
        Decommission In Progress
        Decommissioned
        Entering Maintenance
        In Maintenance
```
- 等待退役节点状态为`decommissioned`（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役
```shell script
# 停止DataNode
hdfs --daemon stop datanode
# 停止ResourceManager
yarn --daemon stop nodemanager
```
- 如果数据不均衡，可以用命令实现集群的再平衡
```shell script
.../sbin/start-balancer.sh -threshold 10
```
# HDFS——存储优化
## 纠删码
HDFS默认情况下，一个文件有3个副本，这样提高了数据的可靠性，但也带来了2倍的冗余开销。Hadoop3.x引入了`纠删码`，采用计算的方式，可以节省约50％左右的存储空间。
### 纠删码原理
```
                                                     -------------------------
                                                    | 一个文件300MB（3个副本） |
                                                     -------------------------
          _______________________________________________________|____________________________________________
         |                         |                        |                       |                         |
         |                         |                        |                       |                         |
  -------V--------         --------V-------         --------V-------         -------V--------         --------V------- 
 |    hadoop1     |       |    hadoop2     |       |    hadoop3     |       |    hadoop4     |       |    hadoop5     |      
 |   ----------   |       |   ----------   |       |                |       |   ----------   |       |                |
 |  |300M副本1 |  |       |  |300M副本2 |   |       |                |       |  |300M副本3 |  |       |                |
 |   ----------   |       |   ----------   |       |                |       |   ----------   |       |                |
 |   -----------  |       |   -----------  |       |   -----------  |       |   -----------  |       |   -----------  |
 |  | 数据单元1 |  |       |  | 数据单元2 |  |       |  | 数据单元3 | |       |  | 数据单元4  | |       |  | 数据单元5 |  |
 |   -----------  |       |   -----------  |       |   -----------  |       |   -----------  |       |   -----------  |
  ----------------         ----------------         ----------------         ----------------         ---------------- 
         ^                        ^                          ^                       ^                        ^
         | 100MB                  | 100MB                    | 100MB                 | 100MB                  | 100MB
         |________________________|__________________________|_______________________|________________________|
                                                             |
                                       --------------------------------------------
                                      | 一个文件300MB(拆分成3个数据单元 + 2个校验单元)|
                                      | 存储上只比自己大小多了两个校验单元            | 
                                       --------------------------------------------
```
#### 纠删码操作相关的命令
```shell script
# 查看纠删码操作指令
hdfs ec

# 响应结果
Usage: bin/hdfs ec [COMMAND]
          [-listPolicies]
          [-addPolicies -policyFile <file>]
          [-getPolicy -path <path>]
          [-removePolicy -policy <policy>]
          [-setPolicy -path <path> [-policy <policy>] [-replicate]]
          [-unsetPolicy -path <path>]
          [-listCodecs]
          [-enablePolicy -policy <policy>]
          [-disablePolicy -policy <policy>]
          [-help <command-name>].
```
#### 查看当前支持的纠删码策略
```shell script
# 查看纠删码策略列表
hdfs ec -listPolicies

# 响应结果
Erasure Coding Policies:
ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=DISABLED

ErasureCodingPolicy=[Name=RS-3-2-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=3, numParityUnits=2]], CellSize=1048576, Id=2], State=DISABLED

ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=ENABLED
 
ErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLED

ErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLED
```
#### 纠删码策略解释
- RS-3-2-1024k：使用`RS编码`，每3个数据单元，生成2个校验单元，共5个单元，也就是说：这5个单元中，只要有任意的3个单元存在（不管是数据单元还是校验单元，只要总数=3），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。
- RS-10-4-1024k：使用`RS编码`，每10个数据单元（cell），生成4个校验单元，共14个单元，也就是说：这14个单元中，只要有任意的10个单元存在（不管是数据单元还是校验单元，只要总数=10），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。
- RS-6-3-1024k：使用`RS编码`，每6个数据单元，生成3个校验单元，共9个单元，也就是说：这9个单元中，只要有任意的6个单元存在（不管是数据单元还是校验单元，只要总数=6），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。
- RS-LEGACY-6-3-1024k：策略和上面的RS-6-3-1024k一样，只是`编码算法`用的是`RS-LEGACY`。 
- XOR-2-1-1024k：使用`XOR编码`（速度比RS编码快），每2个数据单元，生成1个校验单元，共3个单元，也就是说：这3个单元中，只要有任意的2个单元存在（不管是数据单元还是校验单元，只要总数= 2），就可以得到原始数据。每个单元的大小是1024k=1024*1024=1048576。
### 纠删码案例实战
- 纠删码策略是给具体一个路径设置。所有往此路径下存储的文件，都会执行此策略。
- 默认只开启对RS-6-3-1024k策略的支持，如要使用别的策略需要提前启用。
#### 需求
将/input目录设置为RS-3-2-1024k策略
#### 具体步骤
- 开启对RS-3-2-1024k策略的支持
```shell script
# 使用命令行开启纠删码
hdfs ec -enablePolicy  -policy RS-3-2-1024k
```
- 在HDFS创建input目录，并设置`RS-3-2-1024k`策略
```shell script
# 创建目录
hdfs dfs -mkdir /input
# 把input目录设置为RS-3-2-1024k纠删码策略
hdfs ec -setPolicy -path /input -policy RS-3-2-1024k
```
- 上传文件，并查看文件编码后的存储情况
```shell script
# 注意：上传文件需要大于2M才能看出效果。（低于2M，只有一个数据单元和两个校验单元）
hdfs dfs -put web.log /input
```
- 查看存储路径的数据单元和校验单元，并作破坏实验
## 异构存储(冷热分离)
异构存储主要解决，不同的数据，存储在不同类型的硬盘中，达到最佳性能的问题。
```
                                           数据冷热分离

      
           正在用                     经常用                  不常用                  永久保存
      ----------------          ----------------         ----------------         ---------------- 
     |     hadoop1    |        |    hadoop2     |       |    hadoop3     |       |    hadoop4     |
     | -------------- |        |  -----------   |       |   -----------  |       |   -----------  |
     ||硬盘，内存镜像| |        | |硬盘，固态  |  |       |  |硬盘，机械 |  |       |  |硬盘，破旧 |  |
     | -------------- |        |  -----------   |       |   -----------  |       |   -----------  |
      ----------------          ----------------         ----------------         ---------------- 
```
### 存储类型
- RAM_DISK：内存镜像文件系统
- SSD：SSD固态硬盘
- DISK：普通磁盘`在HDFS中如果没有主动声明数据目录存储类型默认都是DISK`
- ARCHIVE：没有特指哪种存储介质，主要是指`计算能力比较弱`，`存储密度比较高`的存储介质，用来解决数据量的`容量扩增`问题，`一般用于归档`
### 存储策略
存储策略对文件的I/O速度由Lazy_Persist向Cold递减

| 策略ID | 策略名称     | 副本分布            | 说明       |
| ------ | ------------ | ------------------- | ------------------- |
| 15     | Lazy_Persist | RAM_DISK:1,DISK:n-1 |一个副本保存在内存中，其余副本保存在磁盘上|
| 12     | All_SSD      | SSD:n               |所有副本都保存在SSD中|
| 10     | One_SDD      | SSD:1,DISK:n-1      |一个副本保存在SSD，其余保存在磁盘上|
| 7      | Hot(default) | DISK:n              |所有副本都保存在磁盘上|
| 5      | Warm         | DISK:1,ARCHIVE:n-1  |一个副本保存在磁盘上，其余副本保存在归档存储上|
| 2      | Cold         | ARCHIVE:n           |所有副本都保存在归档存储上|
### 异构存储Shell操作
```shell script
# 查看当前有哪些存储策略可以用
hdfs storagepolicies -listPolicies

# 为指定路径（数据存储目录）设置指定的存储策略
hdfs storagepolicies -setStoragePolicy -path <DIRECTORY_OR_FILE> -policy <STORAGE_POLICY>

# 获取指定路径（数据存储目录或文件）的存储策略
hdfs storagepolicies -getStoragePolicy -path <DIRECTORY_OR_FILE>

# 取消存储策略；执行改命令之后该目录或者文件，以其上级的目录为准，如果是根目录，那么就是HOT
hdfs storagepolicies -unsetStoragePolicy -path <DIRECTORY_OR_FILE>

# 查看文件块的分布
hdfs fsck <DIRECTORY_OR_FILE> -files -blocks -locations

# 为指定路径（数据存储目录）按照存储策略自行移动文件块
hdfs mover <DIRECTORY_OR_FILE>

# 查看集群节点
hadoop dfsadmin -report
```
### 存储策略测试
#### HOT
#### WARM
#### COLD
#### ONE_SSD
#### ALL_SSD
#### LAZY_PERSIST
如果想要使用Lazy_Persist存储策略还需要配置“dfs.datanode.max.locked.memory”，“dfs.block.size”参数。

- 可能会造成Lazy_Persist策略失效的原因：
    - 当客户端所在的DataNode节点没有RAM_DISK时，则会写入DataNode节点的DISK磁盘。
    - 当客户端所在的DataNode有RAM_DISK，但“dfs.datanode.max.locked.memory”参数值未设置或者设置过小（小于“dfs.block.size”参数值）时，则会写入DataNode节点的DISK磁盘。
    - “dfs.datanode.max.locked.memory”还会受限于Linux系统本身的“max locked memory(默认：64KB)”影响，如果参数设置过大会报错：Cannot start datanode because the configured max locked memory size (dfs.datanode.max.locked.memory)
    - 可以使用命令`ulimit -a`来查询系统本身的max locked memory大小
# HDFS故障排除
## NameNode故障处理
### 需求
NameNode进程损坏，并且存储的元数据也丢失了，思考如何恢复NameNode？
### 解决方案
- 拷贝SecondaryNameNode中数据到原NameNode存储数据目录
```shell script
scp -r ..../hadoop-3.1.3/data/dfs/namesecondary/* ..../hadoop-3.1.3/data/dfs/name/
```
- 重新启动NameNode
```shell script
hdfs --deamon start namenode
```
## 集群安全模式&磁盘修复
集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。
### 安全模式
hdfs只接受读数据请求，而不接受删除、修改等变更请求
### 进入安全模式场景(集群启动时)
- NameNode在加载`镜像文件`和`编辑日志`期间处于安全模式
- NameNode在接收`DataNode注册`时，处于安全模式
### 退出安全模式
- dfs.namenode.safemode.min.datanodes：最小可用DataNode数量`默认：0`（至少有一个DataNode节点）
- dfs.namenode.safemode.threshold-pct：副本数达到最小要求的block占系统总block数的百分比，默认0.999f。（只允许丢一个块）
- dfs.namenode.safemode.extension：稳定时间，默认值30000毫秒，即NameNode启动后超过30秒
### 基本语法
```shell script
# 查看安全模式状态
hdfs dfsadmin -safemode get

# 进入安全模式状态
hdfs dfsadmin -safemode enter

# 离开安全模式状态
hdfs dfsadmin -safemode leave

# 等待安全模式状态
hdfs dfsadmin -safemode wait
```
### 磁盘修复
#### 需求
数据块损坏，进入安全模式，如何处理
#### 解决方案
- 修复数据块：这个需要专业的修复团队干（得花钱）；注意：发现数据块损坏后不要进行操作，立即断电关机，等待专业修复团队；
- 删除数据块：如果是一些不重要的数据可以直接删除NameNode上的数据块信息即可
### 慢磁盘监控
“慢磁盘”是指写入数据非常慢的一类磁盘。当机器`运行时间长`了、`跑的任务多`了，磁盘的读写性能自然会退化，严重时就会出现`写入数据延时`的问题。
#### 如何发现慢磁盘
正常在HDFS上创建一个目录，只需要不到1s的时间。如果你发现创建目录超过1分钟及以上，而且这个现象并不是每次都有。只是偶尔慢了一下，就很有可能存在慢磁盘。
可以采用如下方法找出是哪块磁盘慢：
##### 通过心跳未联系时间
一般出现慢磁盘现象，会影响到DataNode与NameNode之间的`心跳`。正常情况心跳时间间隔是3s。超过3s说明有异常。
##### fio命令，测试磁盘的读写性能 
```shell script
# 安装fio功能
sudo yum install -y fio

# 顺序读测试
sudo fio -filename=/home/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r

# 顺序写测试
sudo fio -filename=/home/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w

# 随机写测试
sudo fio -filename=/home/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw

# 混合随机读写
sudo fio -filename=/home/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop
```
### 小文件归档
100个1k的小文件和100个128M的小文件占用NN内存大小一样，因为每个文件块的元数据占用NN的内存都是150byte
#### HDFS存储小文件弊端
每个文件均按block存储，每个block的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。
#### 解决存储小文件办法之一
`HDFS存档文件`或`HAR文件`，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存。
#### 案例实操
```shell script
# 归档文件：把/sanguo目录里面的所有文件归档成一个叫sanguo.har的归档文件，并把归档后文件存储到/output路径下。
hadoop archive -archiveName sanguo.har -p /sanguo /output

# 查看归档：查看归档下的列表
hadoop fs -ls /output/input.har

# 查看归档：查看归档下的小文件列表
hadoop fs -ls har:///output/input.har

# 解归档文件：把/sanguo目录下的sanguo.har归档解压到/output目录下
hadoop fs -cp har:///output/sanguo.har/* /output
```
# HDFS集群迁移
## Apache与Apache集群间的数据拷贝
- scp实现两个远程主机之间的文件复制`有配置ssh`
```shell script
# 推 push
scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt

# 拉 pull
scp -r root@hadoop103:/user/atguigu/hello.txt  hello.txt

# 是通过本地主机中转实现两个远程主机的文件复制
scp -r root@cluster1001:.../sanguo/liubei.txt root@cluster1004:.../sanguo/
```
- 采用distcp命令实现两个Hadoop集群之间的递归数据复制`无配置ssh`
```shell script
hadoop distcp hdfs://cluster1001:8020/sanguo/liubei.txt hdfs://cluster1004:8020/xiyouji/liubei.txt
```
## Apache与CDH集群间的数据拷贝
- 准备好Apache和CDH集群
- 在Apache集群中的hosts加上CDH集群各个节点的域名，并分发到各个节点上。
- 因为集群都是HA模式，所以需要在apache集群上配置CDH集群,让distcp能识别出CDH的nameservice
- 修改CDH hosts，加上Apache集群各节点的域名，并分发到各个节点上
- 注意：如果Apache和CDH集群有域名重复，需要关闭域名访问，使用IP访问
- 使用hadoop distcp命令进行迁移，-Dmapred.job.queue.name指定队列，默认是default队列。上面配置集群都配了的话，那么在CDH和apache集群下都可以执行这个命令
# MapReduce生产经验
## MapReduce跑得慢的原因
MapReduce程序效率的瓶颈在于两点：
### 计算机性能
- CPU
- 内存
- 磁盘
- 网络
### I/O操作优化
- 数据倾斜
- Map运行时间太长，导致Reduce等待过久
- 小文件过多
## MapReduce常用调优参数
### 自定义分区，减少数据倾斜
自定义类，继承Partitioner抽象类，重写getPartition方法
### 减少溢写的次数
- mapreduce.task.io.sort.mb：Shuffle的环形缓冲区大小（默认：100MB），可以适当提高到200MB
- mapreduce.map.sort.spill.percent：环形缓冲区溢出的阈值，默认80%，可以提高到90%
### 增加每次Merge合并的个数
mapreduce.task.io.sort.factor：每次分区合并的分区数量（默认：10），可以适当提高到20
### 在不影响业务结果前提下采用Combiner
job.setCombinerClass(xxxCombiner.class)
### 为了减少磁盘I/O和网络传输，可以采用Snappy或LZO压缩
```java
// 开启map端输出压缩
config.setBoolean("mapreduce.map.output.compress", true);
// 设置map端输出压缩方式
config.setClass("mapreduce.map.output.compress.codec", SnappyCodec.class, CompressionCodec.class);
```
### 提高MapTask的内存上限
mapreduce.map.memory.mb：MapTask内存大小（默认：1024MB），可以根据Block大小适当调高`原则是block=128MB，MapTask=1G`
### 增加MapTask堆内存，减少内存回收次数
mapreduce.map.java.opts：控制MapTask堆内存大小，同步MapTask的内存上限即可。（如果内存不足会报java.lang.OutOfMemoryError）
### 增加MapTask的CPU核数，提高计算速度
mapreduce.map.cpu.vcores：单个MapTask拥有的CPU核数（默认：1），计算密集型任务可以适当提高上限
### MapTask异常重试
mapreduce.map.maxattempts：每个MapTask的最大异常重试次数，一旦重试次数超过该阈值，则认为MapTask运行失败（默认：4），根据机器性能适当提高
### 提高Reduce拉去Map数据速度
mapreduce.reduce.shuffle.parallecopies：每个Reduce去Map拉去数据的并行数（默认：5），可以根据机器CPU核数适当提高
### 提高Buffer占Reduce内存比例
mapreduce.reduce.shuffle.buffer.percent：Buffer占用Reduce的比例（默认：0.7），可以适当提高到0.8
### 提高ReduceTask的内存上限
mapreduce.reduce.memory.mb：ReduceTask内存大小（默认：1024MB），可以根据Block大小适当调高`原则是block=128MB，MapTask=1G`
### 增加ReduceTask堆内存，减少内存回收次数
mapreduce.reduce.java.opts：控制ReduceTask堆内存大小，同步ReduceTask的内存上限即可。（如果内存不足会报java.lang.OutOfMemoryError）
### 增加ReduceTask的CPU核数，提高计算速度
mapreduce.reduce.cpu.vcores：单个ReduceTask拥有的CPU核数（默认：1），计算密集型任务可以适当提高上限
### ReduceTask异常重试
mapreduce.reduce.maxattempts：每个ReduceTask的最大异常重试次数，一旦重试次数超过该阈值，则认为ReduceTask运行失败（默认：4），根据机器性能适当提高
### 提前启动ReduceTask
mapreduce.job.reduce.showstart.completedmaps：当MapTask完成的比例达到该阈值后才会为ReduceTask申请资源（默认：0.05），可以适当降低
### 优化ReduceTask超时设置
mapreduce.task.timeout：如果一个ReduceTask在一段时间内没有读取到新数据，也没有输出数据，则认为该Task为Block状态，为了防止Task永久占用资源，则设置了超时强制退出的时间（默认：600000ms），根据程序处理每条输入数据的时间来适当设置
### 尽量缺省Reduce操作
因为Reduce过程包含Shuffle中复杂的排序以及归并等处理，而这些处理是非常耗时操作
## MapReduce数据倾斜问题
### 数据倾斜现象
- 数据频率倾斜：某一个区域的数据量要远远大于其他区域。
- 数据大小倾斜：部分记录的大小远远大于平均值。
### 减少数据倾斜
- 首先检查是否`空值过多`造成的数据倾斜：生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合。
- 能在map阶段提前处理：最好先在Map阶段处理。如：Combiner、MapJoin
- 设置多个reduce个数，自定义分区逻辑
# Yarn生产经验
## 常用的调优参数
### ResourceManager相关
```shell script
# ResourceManager处理调度器请求的线程数量
yarn.resourcemanager.scheduler.client.thread-count

# 配置调度器
yarn.resourcemanager.scheduler.class
```
### NameNode相关
```shell script
# NodeManager使用内存数
yarn.nodemanager.resource.memory-mb

# NodeManager为系统保留多少内存，和上一个参数二者取一即可
yarn.nodemanager.resource.system-reserved-memory-mb

# NodeManager使用CPU核数
yarn.nodemanager.resource.cpu-vcores

# 是否将虚拟核数当作CPU核数
yarn.nodemanager.resource.count-logical-processors-as-cores

# 虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2
yarn.nodemanager.resource.pcores-vcores-multiplier

# 是否让yarn自己检测硬件进行配置
yarn.nodemanager.resource.detect-hardware-capabilities

# 是否开启物理内存检查限制container
yarn.nodemanager.pmem-check-enabled

# 是否开启虚拟内存检查限制container
yarn.nodemanager.vmem-check-enabled

# 虚拟内存物理内存比例
yarn.nodemanager.vmem-pmem-ratio
```

# Container相关
```shell script
# 容器最小内存
yarn.scheduler.minimum-allocation-mb

# 容器最大内存
yarn.scheduler.maximum-allocation-mb

# 容器最小核数
yarn.scheduler.minimum-allocation-vcores

# 容器最大核数
yarn.scheduler.maximum-allocation-vcores
```
## 容量调度器使用

## 公平调度器使用
# 综合调优
## 小文件优化方法
### 需求
- HDFS上每个文件都要在NameNode上创建对应的元数据`约为150byte`，这样当小文件比较多的时候，就会产生很多的元数据文件：一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。
- 数据多切片过程每个文件都是独立的，所以会`造成启动过多的MapTask`。每个MapTask处理的数据量小，`导致MapTask的处理时间比启动时间还小`，白白消耗资源。
### 解决方案
#### 数据源头
在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS
#### 存储方向（hadoop archive）
是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用
#### 计算方向(
##### CombineTextInputFormat(减少切片)
CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。
##### 开启uber模式，实现JVM重用
默认情况下，每个Task任务都需要启动一个JVM来运行，如果Task任务计算的数据量很小，让一个JVM并行处理同一个job中的多个Task。
- 开启uber模式，在mapred-site.xml中添加如下配置
```xml
<!--  开启uber模式，默认关闭 -->
<property>
  	<name>mapreduce.job.ubertask.enable</name>
  	<value>true</value>
</property>

<!-- uber模式中最大的mapTask数量，可向下修改  --> 
<property>	
  	<name>mapreduce.job.ubertask.maxmaps</name>
  	<value>9</value>
</property>
<!-- uber模式中最大的reduce数量，可向下修改 -->
<property>
  	<name>mapreduce.job.ubertask.maxreduces</name>
  	<value>1</value>
</property>
<!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 -->
<property>
  	<name>mapreduce.job.ubertask.maxbytes</name>
  	<value></value>
</property>
```
- 分发配置
## 测试MapReduce计算性能
使用Sort程序评测MapReduce；注意：一个虚拟机不超过150G磁盘尽量不要执行这段代码
```shell script
# 使用RandomWriter来产生随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数
hadoop jar ..../hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data

# 执行Sort程序
hadoop jar ..../hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data

# 
hadoop jar ..../hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```
## 企业开发案例
### HDFS参数调优
- 修改hadoop_env.sh配置文件
```
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```
- 修改hdfs-site.xml
```xml
<!-- NameNode有一个工作线程池，默认值是10 -->
<property>
    <name>dfs.namenode.handler.count</name>
    <value>21</value>
</property>
```
- 修改core-site.xml
```xml
<!-- 配置垃圾回收时间为60分钟 -->
<property>
    <name>fs.trash.interval</name>
    <value>60</value>
</property>
```
### MapReduce参数调优
- 修改mapred-site.xml
```xml
<!-- 环形缓冲区大小，默认100m -->
<property>
  <name>mapreduce.task.io.sort.mb</name>
  <value>100</value>
</property>

<!-- 环形缓冲区溢写阈值，默认0.8 -->
<property>
  <name>mapreduce.map.sort.spill.percent</name>
  <value>0.80</value>
</property>

<!-- merge合并次数，默认10个 -->
<property>
  <name>mapreduce.task.io.sort.factor</name>
  <value>10</value>
</property>

<!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts -->
<property>
  <name>mapreduce.map.memory.mb</name>
  <value>-1</value>
  <description>The amount of memory to request from the scheduler for each    map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.
  </description>
</property>

<!-- matask的CPU核数，默认1个 -->
<property>
  <name>mapreduce.map.cpu.vcores</name>
  <value>1</value>
</property>

<!-- matask异常重试次数，默认4次 -->
<property>
  <name>mapreduce.map.maxattempts</name>
  <value>4</value>
</property>

<!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 -->
<property>
  <name>mapreduce.reduce.shuffle.parallelcopies</name>
  <value>5</value>
</property>

<!-- Buffer大小占Reduce可用内存的比例，默认值0.7 -->
<property>
  <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
  <value>0.70</value>
</property>

<!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 -->
<property>
  <name>mapreduce.reduce.shuffle.merge.percent</name>
  <value>0.66</value>
</property>

<!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts -->
<property>
  <name>mapreduce.reduce.memory.mb</name>
  <value>-1</value>
  <description>The amount of memory to request from the scheduler for each    reduce task. If this is not specified or is non-positive, it is inferred
    from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.
    If java-opts are also not specified, we set it to 1024.
  </description>
</property>

<!-- reducetask的CPU核数，默认1个 -->
<property>
  <name>mapreduce.reduce.cpu.vcores</name>
  <value>2</value>
</property>

<!-- reducetask失败重试次数，默认4次 -->
<property>
  <name>mapreduce.reduce.maxattempts</name>
  <value>4</value>
</property>

<!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 -->
<property>
  <name>mapreduce.job.reduce.slowstart.completedmaps</name>
  <value>0.05</value>
</property>

<!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 -->
<property>
  <name>mapreduce.task.timeout</name>
  <value>600000</value>
</property>
```
### Yarn参数调优
- 修改yarn-site.xml配置参数如下
```xml
<!-- 选择调度器，默认容量 -->
<property>
	<description>The class to use as the resource scheduler.</description>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） -->
<property>
	<description>Number of threads to handle scheduler interface.</description>
	<name>yarn.resourcemanager.scheduler.client.thread-count</name>
	<value>8</value>
</property>

<!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 -->
<property>
	<description>Enable auto-detection of node capabilities such as
	memory and CPU.
	</description>
	<name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
	<value>false</value>
</property>

<!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 -->
<property>
	<description>Flag to determine if logical processors(such as
	hyperthreads) should be counted as cores. Only applicable on Linux
	when yarn.nodemanager.resource.cpu-vcores is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true.
	</description>
	<name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
	<value>false</value>
</property>

<!-- 虚拟核数和物理核数乘数，默认是1.0 -->
<property>
	<description>Multiplier to determine how to convert phyiscal cores to
	vcores. This value is used if yarn.nodemanager.resource.cpu-vcores
	is set to -1(which implies auto-calculate vcores) and
	yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.
	</description>
	<name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
	<value>1.0</value>
</property>

<!-- NodeManager使用内存数，默认8G，修改为4G内存 -->
<property>
	<description>Amount of physical memory, in MB, that can be allocated 
	for containers. If set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically calculated(in case of Windows and Linux).
	In other cases, the default is 8192MB.
	</description>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>4096</value>
</property>

<!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 -->
<property>
	<description>Number of vcores that can be allocated
	for containers. This is used by the RM scheduler when allocating
	resources for containers. This is not used to limit the number of
	CPUs used by YARN containers. If it is set to -1 and
	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
	automatically determined from the hardware in case of Windows and Linux.
	In other cases, number of vcores is 8 by default.</description>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>4</value>
</property>

<!-- 容器最小内存，默认1G -->
<property>
	<description>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>1024</value>
</property>

<!-- 容器最大内存，默认8G，修改为2G -->
<property>
	<description>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.
	</description>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>

<!-- 容器最小CPU核数，默认1个 -->
<property>
	<description>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.
	</description>
	<name>yarn.scheduler.minimum-allocation-vcores</name>
	<value>1</value>
</property>

<!-- 容器最大CPU核数，默认4个，修改为2个 -->
<property>
	<description>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an
	InvalidResourceRequestException.</description>
	<name>yarn.scheduler.maximum-allocation-vcores</name>
	<value>2</value>
</property>

<!-- 虚拟内存检查，默认打开，修改为关闭 -->
<property>
	<description>Whether virtual memory limits will be enforced for
	containers.</description>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>

<!-- 虚拟内存和物理内存设置比例,默认2.1 -->
<property>
	<description>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.
	</description>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>
```
